{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "865ab02a-a7f1-4d95-8359-3788ade92ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.10.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Many functions within this notebook were developed using the available online tensorflow tutorial and then modified for personal use and flexibility\n",
    "    # https://www.tensorflow.org/tutorials/structured_data/time_series\n",
    "\n",
    "# Libraries for the rest of the code\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tkinter import *\n",
    "import json\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "#from keras.utils import generic_utils\n",
    "#from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import LSTM, Dense, GRU, RNN, Bidirectional, Dropout\n",
    "from platform import python_version\n",
    "from tensorflow.keras.preprocessing import timeseries_dataset_from_array \n",
    "#from tensorflow import keras as ks\n",
    "\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Print tf version\n",
    "print('TF version:' ,tf.version.VERSION, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94b18556-0310-48bf-a80f-115e31d8335e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Detected:  1\n",
      "Current device: [(PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), {'device_name': 'NVIDIA GeForce RTX 4060 Laptop GPU', 'compute_capability': (8, 9)})]\n",
      "TF built with GPU support: True\n",
      "TF built with cuda support: True \n",
      "\n",
      "GPU is available!\n",
      "\n",
      "Current GPU usage by TF: {'current': 2048, 'peak': 2560}\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is detected and check cuda (1) gpu device type (2) compute capability details\n",
    "list_device = [(x, tf.config.experimental.get_device_details(x)) for x in tf.config.list_physical_devices('GPU')]\n",
    "# Check if TF package is built with Cuda support\n",
    "tf_built_with_cuda_support = tf.test.is_built_with_cuda()\n",
    "tf_built_with_gpu_support = tf.test.is_built_with_gpu_support()\n",
    "\n",
    "print(\"GPUs Detected: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print('Current device:', list_device)\n",
    "print('TF built with GPU support:', tf_built_with_gpu_support)\n",
    "print('TF built with cuda support:', tf_built_with_cuda_support, '\\n')\n",
    "\n",
    "# Further test gpu by a simple addition on the gpu device\n",
    "class MyTest(tf.test.TestCase):\n",
    "\n",
    "    def test_add_on_gpu(self):\n",
    "        if not tf.test.is_built_with_gpu_support():\n",
    "            self.skipTest(\"test is only applicable on GPU\")\n",
    "            print('GPU is not found!')\n",
    "        with tf.device(\"GPU:0\"):\n",
    "            self.assertEqual(tf.math.add(1.0, 2.0), 3.0)\n",
    "            print('GPU is available!')\n",
    "\n",
    "# Run the test on gpu\n",
    "myinstance = MyTest()\n",
    "myinstance.test_add_on_gpu()\n",
    "\n",
    "# If gpu is seen, check the memory usage by TF\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0: \n",
    "    print('\\nCurrent GPU usage by TF:', tf.config.experimental.get_memory_info('GPU:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d522d23d-7f19-434f-8df6-2347c973b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window Generator class to control input and outputs (processes each well separately and combines batches later as long as well_names are present in provided excel sheet)\n",
    "\n",
    "# WindowGenerator class for organizing input-output structure of data for training\n",
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, \n",
    "                 shift, input_columns=None, label_columns=None,shuffle_data = False,batch_number = 32,\n",
    "                 train_df=None, val_df=None, test_df=None): \n",
    "        \n",
    "        # Normalized data\n",
    "        self.train_df_norm = train_df[input_columns]\n",
    "        self.val_df_norm = val_df[input_columns]\n",
    "        self.test_df_norm = test_df[input_columns]\n",
    "        \n",
    "        # Preprocessed raw dataset (includes normalized input columns)\n",
    "        self.train_df_raw = train_df  \n",
    "        self.val_df_raw = val_df\n",
    "        self.test_df_raw = test_df\n",
    "        \n",
    "        # Shuffling and batch size options in make_dataset method\n",
    "        self.shuffle_data = shuffle_data\n",
    "        self.batch_number = batch_number\n",
    "\n",
    "        # Work out the label column indices.\n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_columns_indices = {name: i for i, name in\n",
    "                                        enumerate(label_columns)}\n",
    "        \n",
    "        self.column_indices = {name: i for i, name in\n",
    "                               enumerate(self.train_df_norm.columns)}\n",
    "        \n",
    "        # Initialize user given input columns\n",
    "        self.input_columns = input_columns\n",
    "\n",
    "        # Work out the window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "\n",
    "        self.total_window_size = input_width + shift\n",
    "\n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])\n",
    "    \n",
    "    # Split time series data into windows (segments) of input and user specified time-shifted output (label) \n",
    "    def split_window(self, features):\n",
    "        inputs = features[:, self.input_slice, :]\n",
    "        labels = features[:, self.labels_slice, :]\n",
    "        if self.label_columns is not None:\n",
    "            labels = tf.stack(\n",
    "                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
    "                axis=-1)\n",
    "\n",
    "        # Slicing doesn't preserve static shape information, so set the shapes manually.\n",
    "        # This way the `tf.data.Datasets` are easier to inspect.\n",
    "        inputs.set_shape([None, self.input_width, None])\n",
    "        labels.set_shape([None, self.label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "\n",
    "    \n",
    "    # For making time series data sets\n",
    "    def make_dataset(self, data):\n",
    "        data = np.array(data, dtype=np.float32)\n",
    "        ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "            data=data,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=self.shuffle_data,\n",
    "            batch_size=self.batch_number,)\n",
    "        \n",
    "            \n",
    "        # Mapped ds (mapped to input, output based on the window size and label size)\n",
    "        ds = ds.map(self.split_window)\n",
    "\n",
    "        # Separate inputs and outputs\n",
    "        inputslist = []\n",
    "        outputslist = []\n",
    "        for index,input_batch in enumerate(ds):\n",
    "            inputslist.append(input_batch[0])\n",
    "            outputslist.append(input_batch[1])\n",
    "\n",
    "        # convert inputs and outputs to tf tensors\n",
    "        tf_inputs = tf.concat(inputslist,axis=0)\n",
    "        tf_outputs = tf.concat(outputslist,axis=0)\n",
    "\n",
    "        # Save tf_inputs and tf_outputs as tf datasets\n",
    "        tf_inputs_dataset = tf.data.Dataset.from_tensor_slices(tf_inputs)\n",
    "        tf_outputs_dataset = tf.data.Dataset.from_tensor_slices(tf_outputs)\n",
    "\n",
    "        # Combine inputs and outputs\n",
    "        model_inputs = tf.data.Dataset.zip(((tf_inputs_dataset), tf_outputs_dataset))\n",
    "        model_inputs = model_inputs.batch(self.batch_number).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "        return model_inputs\n",
    "\n",
    "    \n",
    "    # For accesing training, validation, and test data sets and creating example datasets from train data set\n",
    "    @property\n",
    "    def train(self):\n",
    "        \n",
    "        self.full_batch = []\n",
    "        for i, current_well in enumerate(self.train_df_raw['API/UWI List'].unique()):\n",
    "            self.full_batch.append(self.make_dataset(self.train_df_raw.loc[self.train_df_raw['API/UWI List'] == current_well][self.input_columns]))\n",
    "             \n",
    "        if len(self.full_batch) == 1:\n",
    "            ds = self.full_batch[0]\n",
    "        elif len(self.full_batch) == 2:\n",
    "            ds = self.full_batch[0].concatenate(self.full_batch[1])\n",
    "        elif len(self.full_batch) > 2:\n",
    "            ds = self.full_batch[0].concatenate(self.full_batch[1])\n",
    "            for i,name in enumerate(self.full_batch):\n",
    "                if i >= 2:\n",
    "                    ds = ds.concatenate(self.full_batch[i])\n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        \n",
    "        self.full_batch = []\n",
    "        for i, current_well in enumerate(self.val_df_raw['API/UWI List'].unique()):\n",
    "            self.full_batch.append(self.make_dataset(self.val_df_raw.loc[self.val_df_raw['API/UWI List'] == current_well][self.input_columns]))\n",
    "             \n",
    "        if len(self.full_batch) == 1:\n",
    "            ds = self.full_batch[0]\n",
    "        elif len(self.full_batch) == 2:\n",
    "            ds = self.full_batch[0].concatenate(self.full_batch[1])\n",
    "        elif len(self.full_batch) > 2:\n",
    "            ds = self.full_batch[0].concatenate(self.full_batch[1])\n",
    "            for i,name in enumerate(self.full_batch):\n",
    "                if i >= 2:\n",
    "                    ds = ds.concatenate(self.full_batch[i])\n",
    "        \n",
    "        return ds\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        \n",
    "        self.full_batch = []\n",
    "        for i, current_well in enumerate(self.test_df_raw['API/UWI List'].unique()):\n",
    "            self.full_batch.append(self.make_dataset(self.test_df_raw.loc[self.test_df_raw['API/UWI List'] == current_well][self.input_columns]))\n",
    "             \n",
    "        if len(self.full_batch) == 1:\n",
    "            ds = self.full_batch[0]\n",
    "        elif len(self.full_batch) == 2:\n",
    "            ds = self.full_batch[0].concatenate(self.full_batch[1])\n",
    "        elif len(self.full_batch) > 2:\n",
    "            ds = self.full_batch[0].concatenate(self.full_batch[1])\n",
    "            for i,name in enumerate(self.full_batch):\n",
    "                if i >= 2:\n",
    "                    ds = ds.concatenate(self.full_batch[i])\n",
    "        \n",
    "        return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4118dfff-c837-46ee-848b-d123c5bf781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows the user to select from multiple excel sheets in a given excel file\n",
    "    ## Select the desired sheet or sheets one-by-one and load once finished selecting\n",
    "\n",
    "# This allows some flexibility to keep your data in separate excel sheets and experiment with the models you are developing\n",
    "    ## For example: \n",
    "        ### There are multiple different formations the wells are producing from and you would like to keep them separate\n",
    "        ### You may develop separate models for each formation (separated using different excel sheets)\n",
    "        ### Alternatively, you may develop a single model that can learn to differentiate and handle wells producing from different formations\n",
    "        ### The choice in how you want to handle model development is completely up to you.\n",
    "\n",
    "# Warning: Selecting \"all\" will select all the sheets. Make sure the selected excel sheets are in the correct format. \n",
    "# Warning: This will not work with multiple excel files --> It will work with different excel sheets. You can modify this behavior if needed.\n",
    "\n",
    "def select_sheets(sheet_names = None):\n",
    "    \n",
    "    window = Tk()\n",
    "    window.title('Multiple selection')\n",
    "    \n",
    "    user_selected_sheets = []\n",
    "\n",
    "    # for scrolling vertically\n",
    "    yscrollbar = Scrollbar(window)\n",
    "    yscrollbar.pack(side = RIGHT, fill = Y)\n",
    "\n",
    "    label = Label(window,\n",
    "                  text = \"Select the available sheets below :  \",\n",
    "                  font = (\"Times New Roman\", 10), \n",
    "                  padx = 10, pady = 10)\n",
    "    label.pack()\n",
    "    listbox = Listbox(window, selectmode = \"multiple\", \n",
    "                   yscrollcommand = yscrollbar.set)\n",
    "\n",
    "    # Widget expands horizontally and vertically by assigning both to fill option\n",
    "    listbox.pack(padx = 10, pady = 10,\n",
    "              expand = YES, fill = \"both\")\n",
    "\n",
    "    x = sheet_names\n",
    "\n",
    "    for each_item in range(len(x)):\n",
    "\n",
    "        listbox.insert(END, x[each_item])\n",
    "        listbox.itemconfig(each_item, bg = \"white\")\n",
    "\n",
    "    # Attach listbox to vertical scrollbar\n",
    "    yscrollbar.config(command = listbox.yview)\n",
    "\n",
    "    def selected_item():\n",
    "        # Traverse the tuple returned by the selection method and print corresponding value(s) in the listbox\n",
    "        for i in listbox.curselection():\n",
    "            user_selected_sheets.append(listbox.get(i)) \n",
    "        print('Selected sheets:')\n",
    "        print(user_selected_sheets) # listbox.get(i)\n",
    "        window.destroy()\n",
    "    # Create a button widget and map the command parameter to selected_item function\n",
    "    btn = Button(window, text='Create List', command=selected_item)\n",
    "\n",
    "    # Placing the button and listbox\n",
    "    btn.pack(side='bottom')\n",
    "    listbox.pack()\n",
    "\n",
    "    window.mainloop()\n",
    "\n",
    "    return user_selected_sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "817b3da5-4d4b-44f2-a06b-b2f63ba758aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data after importing it\n",
    "def process_data(Production_data = None, train_percent = None, val_percent = None, test_percent = None, label_columns = None, control_parameters = None):\n",
    "\n",
    "    # Process data before splitting\n",
    "    Data_copy = Production_data\n",
    "\n",
    "    store_wells = []\n",
    "    wells_df = pd.DataFrame(columns=Data_copy.columns)\n",
    "    count = 0\n",
    "    for index,well_name in enumerate(Data_copy['API/UWI List'].unique()):\n",
    "\n",
    "        # Find current well\n",
    "        current_well = Data_copy.loc[Data_copy['API/UWI List'] == well_name].reset_index(drop=True)\n",
    "        # full production history\n",
    "        full_indices = len(current_well)\n",
    "        \n",
    "        # Processed removed production history - Removes the indices before maximum oil rate in a given well.\n",
    "        # Assumes that the oil phase is the stable phase and that the overall production rate starts declining after reaching the maximum oil rate.\n",
    "        max_index_oil = current_well['Monthly Oil'].idxmax()\n",
    "        current_well = current_well[max_index_oil:]\n",
    "        removed_indices = len(current_well)\n",
    "\n",
    "        # Remove wells based on potential index length to be able to select larger window sizes for training, validation, testing\n",
    "        train_indices = int(removed_indices*train_percent)\n",
    "        val_indices = int(removed_indices*val_percent)\n",
    "        test_indices = int(removed_indices*test_percent)\n",
    "\n",
    "        # If (window size >= 20) or more than 6 timesteps removed for removing indices < qo_max (maximum oil rate)\n",
    "        # (full_indices - removed_indices > 6) --> this statement checks the number of indices removed previously (if more than 6 indices were removed it assumes that the well behavior was erratic enough not to be included in the training or testing)\n",
    "        # for val_indices and test_indices, this checks whether there are at least 12 discrete points to validate and test the developed model with\n",
    "            \n",
    "            ## Reason # 1: The training is \"moving window\" based and the window for training and testing was selected to include 6 discrete points. Therefore, when evaluating, if there are 12 points, the window will go through the validation and test sets at least 6 times or more\n",
    "            ## Reason # 2: Wanted to be sure that the model performance evaluation is trustworthy: when tested on lower than 12 points, there is no guarantee that the model is stable enough during hindcasting even if it shows high accuracy.\n",
    "                    #### Ideal scenario: the more data points available for validation and testing, the more trustworthy the model performance evaluation becomes (restricted due to data availability). \n",
    "                ### The Window size should be user defined based on available data for training, validation, and testing.\n",
    "                    #### Ideal scenario: We can select a window size greater than 6 due to high resolution (daily, weekly, etc.) or wells that have been producing longer.\n",
    "                    #### Hint: In a many-to-many or sequence-to-sequence type training, each discrete point's error is determined independently and then accumulated. Therefore, having longer sequences can help train a more accurate model by encouraging the model to minimize the additional errors coming from the additional data points.\n",
    "                ### Hindcasting: when testing for the same well for validating the model's ability to predict future production in the direction of time \n",
    "        if (full_indices - removed_indices > 6) or (test_indices) < 12 or (val_indices) < 12:\n",
    "            count = count + 1\n",
    "            current_well = pd.DataFrame() # Ignores the well by creating an empty dataframe if the above conditions are not met\n",
    "\n",
    "        # If current well is not empty after processing, store the well\n",
    "        if not current_well.empty:\n",
    "            # Replace all '0' with selected number\n",
    "            current_well[label_columns] = current_well[label_columns].replace(0, 0.1)\n",
    "            \n",
    "            # Shift Days column up (Production days on per month) and set days column as future control parameters (days @ timestep= t+1, monthly production @ timestep = t)\n",
    "            current_well = current_well.copy()\n",
    "            current_well[control_parameters] = current_well[control_parameters].shift(-1)\n",
    "            current_well = current_well.dropna().reset_index(drop=True) # Drop NA rows after shifting control parameters (days) up\n",
    "            store_wells.append(current_well)\n",
    "        \n",
    "\n",
    "    # Combine all remaining wells (after preprocessing) and print the results for the user to see \n",
    "    wells_df = pd.concat(store_wells).reset_index(drop=True)    \n",
    "    \n",
    "    print('Number of wells in original data:', len(Data_copy['API/UWI List'].unique()))\n",
    "    print('Number of wells removed:', count)\n",
    "    print('Remaining number of wells:', len(wells_df['API/UWI List'].unique()))\n",
    "    \n",
    "    return wells_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "356cf0f9-d630-43cb-a032-322a0f174979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controls the splitting of production data into training, validation, and test sets.\n",
    "# Assumes the split should be temporal to apply hindcasting to the model.\n",
    "    ### Hindcasting: when testing for the same well for validating the model's ability to predict future production in the direction of time\n",
    "\n",
    "def split_data(DF_WELL,train_percent=0.7,val_percent=0.2,test_percent=0.1):\n",
    "\n",
    "    total_percent = round((train_percent + val_percent + test_percent),2)\n",
    "\n",
    "    if total_percent != 1:\n",
    "        raise Exception(\"Total percentage of train,val,test must be equal to 100%\")\n",
    "    else:\n",
    "        train_data = []\n",
    "        val_data = []\n",
    "        test_data = []\n",
    "\n",
    "        # Slicing for rest of the data after train and val (test data)\n",
    "        train_and_val = train_percent + val_percent # total percent of train + val\n",
    "\n",
    "\n",
    "        # Extract desired input features and split data into train,val,test\n",
    "        for index, current_well in enumerate(DF_WELL['API/UWI List'].unique()):\n",
    "\n",
    "            unique_well = DF_WELL.loc[DF_WELL['API/UWI List'] == current_well]\n",
    "            n = len(unique_well)\n",
    "            train_data.append(unique_well.iloc[0:int(n*train_percent)])\n",
    "            val_data.append(unique_well.iloc[int(n*train_percent):int(n*train_and_val)])\n",
    "            test_data.append(unique_well.iloc[int(n*train_and_val):])\n",
    "\n",
    "        train_df = pd.concat(train_data)\n",
    "        val_df = pd.concat(val_data)\n",
    "        test_df = pd.concat(test_data)\n",
    "        \n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "# Normalize dynamic variables in train, val, test data sets using z-score normalization\n",
    "def normalize_data(train_df, val_df, test_df, plot_cols = ['BP', 'BHP', 'OPR'], norm_method = 'z-score'):\n",
    "    \n",
    "    train_mu = []\n",
    "    train_sigma = []\n",
    "    train_maximum = []\n",
    "    train_minimum = []\n",
    "    train_df_norm = []\n",
    "    val_df_norm = []\n",
    "    test_df_norm = []\n",
    "    \n",
    "    if norm_method == 'z-score':\n",
    "        # Save mean per well\n",
    "        for index, current_well in enumerate(train_df['API/UWI List'].unique()):\n",
    "\n",
    "            train_unique_well = train_df.loc[train_df['API/UWI List'] == current_well]\n",
    "            val_unique_well = val_df.loc[val_df['API/UWI List'] == current_well]\n",
    "            test_unique_well = test_df.loc[test_df['API/UWI List'] == current_well]\n",
    "            train_mean = train_unique_well[plot_cols].mean()\n",
    "            train_std = train_unique_well[plot_cols].std()\n",
    "            train_mu.append(train_mean)\n",
    "            train_sigma.append(train_std)\n",
    "\n",
    "            # Normalize data\n",
    "            train_df_norm.append((train_unique_well[plot_cols]-train_mean)/train_std)\n",
    "            val_df_norm.append((val_unique_well[plot_cols]-train_mean)/train_std)\n",
    "            test_df_norm.append((test_unique_well[plot_cols]-train_mean)/train_std)\n",
    "\n",
    "        for index,value in enumerate(train_df_norm):\n",
    "            train_df_norm[index] = train_df_norm[index].add_suffix('_norm')\n",
    "            val_df_norm[index] = val_df_norm[index].add_suffix('_norm')\n",
    "            test_df_norm[index] = test_df_norm[index].add_suffix('_norm')\n",
    "\n",
    "        train_df_norm = pd.concat(train_df_norm)\n",
    "        val_df_norm = pd.concat(val_df_norm)\n",
    "        test_df_norm = pd.concat(test_df_norm)\n",
    "\n",
    "        train_df = pd.concat([train_df,train_df_norm],axis=1)\n",
    "        val_df = pd.concat([val_df,val_df_norm],axis=1)\n",
    "        test_df = pd.concat([test_df,test_df_norm],axis=1)\n",
    "\n",
    "        return train_df, val_df, test_df, train_mu, train_sigma\n",
    "    \n",
    "    elif norm_method == 'min-max':\n",
    "        \n",
    "        # Save mean per well\n",
    "        for index, current_well in enumerate(train_df['API/UWI List'].unique()):\n",
    "\n",
    "            train_unique_well = train_df.loc[train_df['API/UWI List'] == current_well]\n",
    "            val_unique_well = val_df.loc[val_df['API/UWI List'] == current_well]\n",
    "            test_unique_well = test_df.loc[test_df['API/UWI List'] == current_well]\n",
    "            train_max = train_unique_well[plot_cols].max()\n",
    "            train_min = 0 # train_unique_well[plot_cols].min()\n",
    "            train_maximum.append(train_max)\n",
    "            train_minimum.append(train_min)\n",
    "\n",
    "            # Normalize data\n",
    "            train_df_norm.append((train_unique_well[plot_cols]-train_min)/(train_max-train_min))\n",
    "            val_df_norm.append((val_unique_well[plot_cols]-train_min)/(train_max-train_min))\n",
    "            test_df_norm.append((test_unique_well[plot_cols]-train_min)/(train_max-train_min))\n",
    "\n",
    "        for index,value in enumerate(train_df_norm):\n",
    "            train_df_norm[index] = train_df_norm[index].add_suffix('_norm')\n",
    "            val_df_norm[index] = val_df_norm[index].add_suffix('_norm')\n",
    "            test_df_norm[index] = test_df_norm[index].add_suffix('_norm')\n",
    "\n",
    "        train_df_norm = pd.concat(train_df_norm)\n",
    "        val_df_norm = pd.concat(val_df_norm)\n",
    "        test_df_norm = pd.concat(test_df_norm)\n",
    "\n",
    "        train_df = pd.concat([train_df,train_df_norm],axis=1)\n",
    "        val_df = pd.concat([val_df,val_df_norm],axis=1)\n",
    "        test_df = pd.concat([test_df,test_df_norm],axis=1)\n",
    "\n",
    "        return train_df, val_df, test_df, train_maximum, train_minimum\n",
    "        \n",
    "        \n",
    "    \n",
    "    # If desired, user can develop their own custom normalization method    \n",
    "    elif norm_method == 'custom-method':\n",
    "        print('not available yet...try another normalization method...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb109233-7d75-463d-a34d-b60e6931f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and print model summary and type of model\n",
    "\n",
    "# find the type of model loaded (integrated as part of load model function)\n",
    "def find_model_type(model = None):\n",
    "    \n",
    "    model_type = []\n",
    "    # Loop through each layer of the model to find model type\n",
    "    for layer in model.layers:\n",
    "        if 'Bidirectional' in str(type(layer)):\n",
    "            model_type.append('Bidirectional')\n",
    "            if 'GRU' in str(layer.layer):\n",
    "                model_type.append('GRU')\n",
    "            elif 'LSTM' in str(layer.layer):\n",
    "                model_type.append('LSTM')\n",
    "        elif 'GRU' in str(type(layer)):\n",
    "            model_type.append('GRU')\n",
    "        elif 'LSTM' in str(type(layer)):\n",
    "            model_type.append('LSTM')\n",
    "        elif 'Dense' in str(type(layer)):\n",
    "            model_type.append('Dense')\n",
    "    \n",
    "    model_type = list(set(model_type))\n",
    "\n",
    "    return model_type\n",
    "\n",
    "# Load the model \n",
    "def load_model(model_path = None):\n",
    "    import json\n",
    "    \n",
    "    loaded_model = tf.keras.models.load_model(model_path) # ,custom_objects={\"tf\": tf}\n",
    "    # Print model type\n",
    "    model_type = find_model_type(model = loaded_model)\n",
    "    print('Model Type:')\n",
    "    print(model_type)\n",
    "    \n",
    "    # Print model summary\n",
    "    print('\\nModel Summary:')\n",
    "    loaded_model.summary()\n",
    "    \n",
    "    # Print optimizer configs\n",
    "    print('\\nOptimizer configs:')\n",
    "    print(loaded_model.optimizer.get_config())\n",
    "    \n",
    "    # Print units for each layer\n",
    "    print('\\nLayer units:')\n",
    "    for item in loaded_model.get_config()['layers']:\n",
    "        while 'layer' in item['config']:\n",
    "            item = item['config']['layer']\n",
    "        item  = item['config']\n",
    "        if 'units' in item:\n",
    "            print('units: ',item['units'])\n",
    "    \n",
    "    # If a trainig history exists, print the saved training and validation errors (MAE and MAPE)\n",
    "    # When saving the errors, they are saved as a json file\n",
    "    print('\\nTraining History:')\n",
    "    if os.path.exists(os.path.join(model_path,'history.json')):\n",
    "        history_dict = json.load(open(os.path.join(model_path,'history.json'), 'r'))\n",
    "    \n",
    "        if 'loss' in history_dict.keys():\n",
    "            print('Epochs: ', len(history_dict['loss']))\n",
    "            print('Train-Loss: ', history_dict['loss'][-1])\n",
    "        if 'val_loss' in history_dict.keys():\n",
    "            print('Val-Loss: ', history_dict['val_loss'][-1])\n",
    "        if 'mae' in history_dict.keys():\n",
    "            print('Train-MAE: ', history_dict['mae'][-1])\n",
    "        if 'val_mae' in history_dict.keys():\n",
    "            print('Val-MAPE: ', history_dict['val_mae'][-1])\n",
    "        if 'mape' in history_dict.keys():\n",
    "            print('Train-MAPE: ', history_dict['mape'][-1])\n",
    "        if 'val_mape' in history_dict.keys():\n",
    "            print('Val-MAPE: ', history_dict['val_mape'][-1])\n",
    "    else:\n",
    "        history_dict = []\n",
    "        print(history_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfe9fbcd-aa83-432b-b41b-7a51058517c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Functions for saving and calculating losses per well\n",
    "\n",
    "def loss_compute(y_label = None, y_pred = None, y_label_full = [None], loss_type = 'mse'):\n",
    "    \n",
    "    \"\"\"This function is a general function that calculates losses using the possible list of loss selections: \n",
    "    ['mse','rmse','mae','mape','nmse','nrmse','nmape','nmae','wmape']\n",
    "     \n",
    "     Inputs:\n",
    "     *y_label = ground truth values (array like) \n",
    "     *y_pred = predicted values (array like)\n",
    "     *y_label_full = ground truth values (array like) -> This can be either the full dataset or partial dataset (e.g., train,val and test)\n",
    "     *loss_type = type of desired loss (str or list(str)) --> multiple loss types can be requested in list(str) (e.g. ['mse','rmse','mae'])\n",
    "         **List of possible loss_type options: ['mse','rmse','mae','mape','nmse','nrmse','nmape','nmae','wmape']\n",
    "     \n",
    "     Outputs:\n",
    "     * losses = A dictionary of losses\n",
    "     \n",
    "     EXPLANATION OF y_label_full: \n",
    "     y_label_full is used in calculating Interquartile Range (IQR or 50% percentile).\n",
    "     \n",
    "     EXPLANATION OF IQR: \n",
    "     IQR is integrated into some of the loss metrics such as [nmse,nrmse,nmae] to normalize [mse,rmse,mae] losses.\n",
    "     It allows for more accurate representation of the errors when extreme outliers exist in the dataset. \n",
    "     It is also said to be a better normalized error metric that more accurately represents the robustness of the model in case of existing outliers.\n",
    "     If y_label = train_ground_truth, it is recommended to set y_label_full = train_ground_truth, etc. but, the full dataset can also be used by the user if desired.\n",
    "     \"\"\"\n",
    "    \n",
    "    # Import inside function to sort created loss dictionary in alphabetical order of key items\n",
    "    from collections import OrderedDict\n",
    "    \n",
    "    # *** Initialize the variables ***\n",
    "        \n",
    "    # Possible selections list for loss_type\n",
    "    possible_selections = ['mse','rmse','mae','mape','nmse','nrmse','nmape','nmae','wmape']\n",
    "    \n",
    "    # *** CATCH POSSIBLE ERRORS ***\n",
    "    # Raise Syntax Error if loss_type is not of type 'str'\n",
    "    if (not isinstance(loss_type, list)) & (not isinstance(loss_type, str)): \n",
    "        raise TypeError('loss_type is not of type str: {}.\\nList of valid options: {}'.format(loss_type, possible_selections))\n",
    "    # Raise Syntax Error if loss_type is not in possible_selections list\n",
    "    elif (not isinstance(loss_type, list)):\n",
    "        if (loss_type.lower() not in possible_selections):\n",
    "            raise SyntaxError('Provided loss_type argument is not a valid argument: {}.\\nList of valid options: {}'.format(loss_type, possible_selections))\n",
    "    elif (isinstance(loss_type, list)) & (any(not isinstance(item, str) for item in loss_type)):\n",
    "        raise TypeError('an element in loss_type list is not of type str: {}.\\nList of valid options: {}'.format(loss_type, possible_selections))\n",
    "    elif (isinstance(loss_type, list)) & any(item.lower() not in possible_selections for item in loss_type):\n",
    "        raise SyntaxError('Provided loss_type argument is not a valid argument: {}.\\nList of valid options: {}'.format(loss_type, possible_selections))\n",
    "    # Raise Type error if there the y_label_full is not provided by the user (Required for normalized errors with IQR present)\n",
    "    if any(elem is None for elem in y_label_full):\n",
    "        raise TypeError('an element in y_label_full is None type.\\nThis might cause calculation errors for normlalized errors (e.g., nmae).\\nTo avoid this problem, please provided the desired y_label data (preferably the full data for IQR normalized errors).')\n",
    "        \n",
    "    \n",
    "    # Convert y_label and y_pred to numpy array if not a numpy array\n",
    "    if not isinstance(y_label, np.ndarray):\n",
    "        y_label = np.array(y_label)\n",
    "    if not isinstance(y_pred, np.ndarray):\n",
    "        y_pred = np.array(y_pred)\n",
    "    if not isinstance(y_label_full, np.ndarray):\n",
    "        y_label_full = np.array(y_label_full)\n",
    "    \n",
    "    # Inter quartile range (IQR) (said to be less sensitive to outliers in data), \n",
    "    # Explanation: The IQR is a way to measure the spread of the middle 50% of a dataset. \n",
    "    # It is calculated as the difference between: \n",
    "    # the first quartile* (the 25th percentile) and the third quartile (the 75th percentile) of a dataset.\n",
    "    if any(y_label_full):\n",
    "        q3, q1 = np.percentile(y_label_full, [75 ,25])\n",
    "        iqr = q3 - q1\n",
    "    \n",
    "    # *** Perform one of the loss methods from the possible_selections list ***\n",
    "    try:\n",
    "        \n",
    "        losses = {} # empty dictionary to save the loss value or values\n",
    "        \n",
    "        # Calculates provided list of loss types and saves all inside a dictionary called losses\n",
    "        if isinstance(loss_type, list):\n",
    "            for item in loss_type:\n",
    "                if item.lower() == 'mse':\n",
    "                    losses[item.lower()] = np.square(np.subtract(y_label,y_pred)).mean()\n",
    "                elif item.lower() == 'rmse':\n",
    "                    losses[item.lower()] = np.sqrt(np.square(np.subtract(y_label,y_pred)).mean())\n",
    "                elif item.lower() == 'mae':\n",
    "                    losses[item.lower()]= (np.abs(y_label - y_pred)).mean()\n",
    "                elif item.lower() == 'mape':\n",
    "                    losses[item.lower()] = np.mean(np.abs((y_label - y_pred)/y_label))*100\n",
    "                elif item.lower() == 'nmse':\n",
    "                    losses[item.lower()] = np.square(np.subtract(y_label,y_pred)).mean()/iqr\n",
    "                elif item.lower() == 'nrmse':\n",
    "                    losses[item.lower()] = np.sqrt(np.square(np.subtract(y_label,y_pred)).mean())/iqr\n",
    "                elif item.lower() == 'nmape':\n",
    "                    losses[item.lower()] = np.sum(np.abs((y_label - y_pred)))/np.sum((y_label + y_pred))*100\n",
    "                elif item.lower() == 'nmae':\n",
    "                    if any(y_label_full):\n",
    "                        losses[item.lower()] = (np.abs(y_label - y_pred)).mean()/iqr\n",
    "                elif item.lower() == 'wmape':\n",
    "                    losses[item.lower()] = np.sum(np.abs((y_label - y_pred)))/np.sum(np.abs(y_label))*100\n",
    "        # Calculates requested loss type (single value saved inside loss - not a dictionary)\n",
    "        elif not isinstance(loss_type, list):\n",
    "            if loss_type.lower() == 'mse':\n",
    "                losses[loss_type.lower()] = np.square(np.subtract(y_label,y_pred)).mean()\n",
    "            elif loss_type.lower() == 'rmse':\n",
    "                losses[loss_type.lower()] = np.sqrt(np.square(np.subtract(y_label,y_pred)).mean())\n",
    "            elif loss_type.lower() == 'mae':\n",
    "                losses[loss_type.lower()] = (np.abs(y_label - y_pred)).mean()\n",
    "            elif loss_type.lower() == 'mape':\n",
    "                losses[loss_type.lower()] = np.mean(np.abs((y_label - y_pred)/y_label))*100\n",
    "            elif item.lower() == 'nmse':\n",
    "                losses[loss_type.lower()] = np.square(np.subtract(y_label,y_pred)).mean()/iqr\n",
    "            elif item.lower() == 'nrmse':\n",
    "                losses[loss_type.lower()] = np.sqrt(np.square(np.subtract(y_label,y_pred)).mean())/iqr\n",
    "            elif item.lower() == 'nmape':\n",
    "                losses[loss_type.lower()] = np.sum(np.abs((y_label - y_pred)))/np.sum((y_label + y_pred))*100\n",
    "            elif item.lower() == 'nmae':\n",
    "                if any(y_label_full):\n",
    "                    losses[loss_type.lower()] = (np.abs(y_label - y_pred)).mean()/iqr\n",
    "            elif item.lower() == 'wmape':\n",
    "                losses[loss_type.lower()] = np.sum(np.abs((y_label - y_pred)))/np.sum(np.abs(y_label))*100\n",
    "        return losses\n",
    "    \n",
    "    except:\n",
    "        print('An unexpected exception has occured. Check if y_label and y_pred are provided')\n",
    "\n",
    "\n",
    "\n",
    "def collect_errors(API = None, losses = None):\n",
    "    \n",
    "    # Create the API dictionary\n",
    "    API_DICT = {'API': API}\n",
    "    \n",
    "    # Update the given dictionary with incoming losses dictionary\n",
    "    API_DICT.update(losses)\n",
    "    \n",
    "    return API_DICT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d3f288c-2dba-4095-89f8-089d11b7ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pred_results(input_width = None,\n",
    "                         label_width = None,\n",
    "                         input_columns = None,\n",
    "                         norm_method = 'z-score',\n",
    "                         label_columns = None,\n",
    "                         batch_number = None,\n",
    "                         index_slice_start = 0,\n",
    "                         path_to_save_pred_plots = None, \n",
    "                         model = None,\n",
    "                         model_name = None,\n",
    "                         index_slice = 'all', \n",
    "                         train_df = None, \n",
    "                         val_df = None, \n",
    "                         test_df = None, \n",
    "                         train_mu = None, \n",
    "                         train_sigma = None,\n",
    "                         logscale = 'on',\n",
    "                         normscale = 'off',\n",
    "                         plot_loss = 'wmape',\n",
    "                         split_loss = 'on'):\n",
    "    \n",
    "    # Global plot options\n",
    "    plt.rcParams['figure.figsize'] = [18, 18]\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    \n",
    "    # Empty Error list to collect errors\n",
    "    Error_list = []\n",
    "    Train_Error_list = []\n",
    "    Val_Error_list = []\n",
    "    Test_Error_list = []\n",
    "    \n",
    "    # Extract normalized or real labels and predictions for loss computation based on 'normscale' option provided by user\n",
    "    if normscale == 'on':\n",
    "        norm_values = '_norm'\n",
    "    else:\n",
    "        norm_values = ''\n",
    "    \n",
    "    # Check if directory exists and if not create it\n",
    "    model_type = find_model_type(model = model)\n",
    "\n",
    "    # Choose mode based on label_width\n",
    "    if label_width == 1:\n",
    "        mode = 'many-to-one'\n",
    "    elif label_width > 1:\n",
    "        mode = 'many-to-many'\n",
    "\n",
    "    # Finalize save path\n",
    "    save_path = os.path.join(path_to_save_pred_plots,mode,*model_type,model_name)\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    # Choose slicing options\n",
    "    # Full indices to iterate\n",
    "    if index_slice == 'all':\n",
    "        total_number_of_wells = len(train_df['API/UWI List'].unique())\n",
    "        iterable_indices = list(range(0,total_number_of_wells))\n",
    "    # Partial indices to iterate\n",
    "    elif isinstance(index_slice, int):\n",
    "        iterable_indices = list(range(index_slice_start,index_slice))\n",
    "    \n",
    "    # Empty list to collect all results to save into excel sheet inside created plots folder\n",
    "    Forecast_Results = [] # includes train,val,test together\n",
    "    Forecast_Results_train = []\n",
    "    Forecast_Results_val = []\n",
    "    Forecast_Results_test = []\n",
    "    \n",
    "    # Prepare to save plots with predictions\n",
    "    for index,current_well in enumerate(train_df['API/UWI List'].unique()):\n",
    "        if index in iterable_indices:\n",
    "            # Get the original data (merged_df)\n",
    "            df1 = train_df.loc[train_df['API/UWI List'] == current_well]\n",
    "            df2 = val_df.loc[val_df['API/UWI List'] == current_well]\n",
    "            df3 = test_df.loc[test_df['API/UWI List'] == current_well]\n",
    "            current_list = [df1,df2,df3]\n",
    "            merged_df = pd.concat(current_list)\n",
    "            \n",
    "            # Copy of train_df, val_df, test_df to save results for evaluation in three different excel sheets (post-processing)\n",
    "            train_df_copy = df1[input_width:].copy()\n",
    "            val_df_copy = df2.copy()\n",
    "            test_df_copy = df3.copy()\n",
    "            \n",
    "\n",
    "            # Get the lenght of train,val,test data\n",
    "            train_period = len(df1)\n",
    "            val_period = len(df2) + train_period\n",
    "            test_period = len(df3) + val_period\n",
    "            \n",
    "\n",
    "            # Get predictions\n",
    "            wpred = WindowGenerator(input_width = input_width, label_width = label_width, shift = 1, input_columns = input_columns, label_columns = label_columns, shuffle_data = False,batch_number = batch_number, train_df=merged_df, val_df=val_df.loc[val_df['API/UWI List'] == current_well], test_df=test_df.loc[test_df['API/UWI List'] == current_well])\n",
    "            current_forecast = model.predict(wpred.train)\n",
    "\n",
    "            \n",
    "            # Save predictions based on fluid phase and mode (many-to-one or many-to-many architecture)\n",
    "            # Additionally save cumusum of both actual and predicted values in the same manner\n",
    "            real_forecast = []\n",
    "            colors = []\n",
    "            # Get current forecast info and append to real_forecast\n",
    "            for j,label in enumerate(label_columns):\n",
    "                if 'Oil'.lower() in label.lower():\n",
    "                    phase = label.replace('_norm','')\n",
    "                    \n",
    "                    # CUMSUM of actual oil phase production (independent of mode type)\n",
    "                    train_df_copy.insert(len(train_df_copy.keys()), phase + '_CUMSUM',train_df_copy[phase].cumsum(), False)\n",
    "                    val_df_copy.insert(len(val_df_copy.keys()), phase + '_CUMSUM',val_df_copy[phase].cumsum(), False)\n",
    "                    test_df_copy.insert(len(test_df_copy.keys()), phase + '_CUMSUM',test_df_copy[phase].cumsum(), False)\n",
    "                     \n",
    "                    if mode == 'many-to-one':\n",
    "                        norm_result = current_forecast[:,j]\n",
    "                        extract_result = current_forecast[:,j]*train_sigma[index][phase] + train_mu[index][phase]\n",
    "                        real_forecast.append(extract_result)\n",
    "                        # Extracting results for train, val, test periods to evaluate\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_predictions', extract_result[0:train_period-input_width], False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_predictions',extract_result[train_period-input_width:val_period-input_width], False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_predictions', extract_result[val_period-input_width:test_period-input_width], False)\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_norm' + '_predictions', norm_result[0:train_period-input_width], False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_norm' + '_predictions',norm_result[train_period-input_width:val_period-input_width], False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_norm' + '_predictions', norm_result[val_period-input_width:test_period-input_width], False)\n",
    "                        # Save CUMSUM (PREDICTIONS) for train, val, test periods to evaluate\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_CUMSUM' + '_predictions', extract_result[0:train_period-input_width].cumsum(), False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_CUMSUM' + '_predictions',extract_result[train_period-input_width:val_period-input_width].cumsum(), False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_CUMSUM' + '_predictions', extract_result[val_period-input_width:test_period-input_width].cumsum(), False)\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_norm' + '_CUMSUM' + '_predictions', norm_result[0:train_period-input_width].cumsum(), False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_norm' + '_CUMSUM' + '_predictions',norm_result[train_period-input_width:val_period-input_width].cumsum(), False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_norm' + '_CUMSUM' + '_predictions', norm_result[val_period-input_width:test_period-input_width].cumsum(), False)\n",
    "                    elif mode == 'many-to-many':\n",
    "                        if norm_method == 'z-score':\n",
    "                            norm_result = current_forecast[:,-1][:,j]\n",
    "                            extract_result = current_forecast[:,-1][:,j]*train_sigma[index][phase] + train_mu[index][phase]\n",
    "                            real_forecast.append(extract_result)\n",
    "                        elif norm_method == 'min-max':\n",
    "                            norm_result = current_forecast[:,-1][:,j]\n",
    "                            extract_result = current_forecast[:,-1][:,j]*(train_sigma[index][phase] - train_mu[index]) + train_mu[index]\n",
    "                            real_forecast.append(extract_result)\n",
    "                            \n",
    "                        # Extracting results for train, val, test periods to evaluate\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_predictions', extract_result[0:train_period-input_width], False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_predictions',extract_result[train_period-input_width:val_period-input_width], False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_predictions', extract_result[val_period-input_width:test_period-input_width], False)\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_norm' + '_predictions', norm_result[0:train_period-input_width], False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_norm' + '_predictions',norm_result[train_period-input_width:val_period-input_width], False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_norm' + '_predictions', norm_result[val_period-input_width:test_period-input_width], False)\n",
    "                        # Save CUMSUM (PREDICTIONS) for train, val, test periods to evaluate\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_CUMSUM' + '_predictions', extract_result[0:train_period-input_width].cumsum(), False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_CUMSUM' + '_predictions',extract_result[train_period-input_width:val_period-input_width].cumsum(), False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_CUMSUM' + '_predictions', extract_result[val_period-input_width:test_period-input_width].cumsum(), False)\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_norm' + '_CUMSUM' + '_predictions', norm_result[0:train_period-input_width].cumsum(), False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_norm' + '_CUMSUM' + '_predictions',norm_result[train_period-input_width:val_period-input_width].cumsum(), False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_norm' + '_CUMSUM' + '_predictions', norm_result[val_period-input_width:test_period-input_width].cumsum(), False)\n",
    "                    colors.append('g')\n",
    "                if 'Gas'.lower() in label.lower():\n",
    "                    phase = label.replace('_norm','')\n",
    "                    \n",
    "                    # CUMSUM of actual gas phase production (independent of mode type)\n",
    "                    train_df_copy.insert(len(train_df_copy.keys()), phase + '_CUMSUM',train_df_copy[phase].cumsum(), False)\n",
    "                    val_df_copy.insert(len(val_df_copy.keys()), phase + '_CUMSUM',val_df_copy[phase].cumsum(), False)\n",
    "                    test_df_copy.insert(len(test_df_copy.keys()), phase + '_CUMSUM',test_df_copy[phase].cumsum(), False)\n",
    "                    \n",
    "                    if mode == 'many-to-one':\n",
    "                        norm_result = current_forecast[:,j]\n",
    "                        extract_result = current_forecast[:,j]*train_sigma[index][phase] + train_mu[index][phase]\n",
    "                        real_forecast.append(extract_result)\n",
    "                        # Extracting results for train, val, test periods to evaluate\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_predictions', extract_result[0:train_period-input_width], False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_predictions',extract_result[train_period-input_width:val_period-input_width], False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_predictions', extract_result[val_period-input_width:test_period-input_width], False)\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_norm' + '_predictions', norm_result[0:train_period-input_width], False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_norm' + '_predictions',norm_result[train_period-input_width:val_period-input_width], False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_norm' + '_predictions', norm_result[val_period-input_width:test_period-input_width], False)\n",
    "                        # Save CUMSUM (PREDICTIONS) for train, val, test periods to evaluate\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_CUMSUM' + '_predictions', extract_result[0:train_period-input_width].cumsum(), False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_CUMSUM' + '_predictions',extract_result[train_period-input_width:val_period-input_width].cumsum(), False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_CUMSUM' + '_predictions', extract_result[val_period-input_width:test_period-input_width].cumsum(), False)\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_norm' + '_CUMSUM' + '_predictions', norm_result[0:train_period-input_width].cumsum(), False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_norm' + '_CUMSUM' + '_predictions',norm_result[train_period-input_width:val_period-input_width].cumsum(), False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_norm' + '_CUMSUM' + '_predictions', norm_result[val_period-input_width:test_period-input_width].cumsum(), False)\n",
    "                    elif mode == 'many-to-many':\n",
    "                        if norm_method == 'z-score':\n",
    "                            norm_result = current_forecast[:,-1][:,j]\n",
    "                            extract_result = current_forecast[:,-1][:,j]*train_sigma[index][phase] + train_mu[index][phase]\n",
    "                            real_forecast.append(extract_result)\n",
    "                        elif norm_method == 'min-max':\n",
    "                            norm_result = current_forecast[:,-1][:,j]\n",
    "                            extract_result = current_forecast[:,-1][:,j]*(train_sigma[index][phase] - train_mu[index]) + train_mu[index]\n",
    "                            real_forecast.append(extract_result)\n",
    "                        # Extracting results for train, val, test periods to evaluate\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_predictions', extract_result[0:train_period-input_width], False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_predictions',extract_result[train_period-input_width:val_period-input_width], False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_predictions', extract_result[val_period-input_width:test_period-input_width], False)\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_norm' + '_predictions', norm_result[0:train_period-input_width], False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_norm' + '_predictions',norm_result[train_period-input_width:val_period-input_width], False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_norm' + '_predictions', norm_result[val_period-input_width:test_period-input_width], False)\n",
    "                        # Save CUMSUM (PREDICTIONS) for train, val, test periods to evaluate\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_CUMSUM' + '_predictions', extract_result[0:train_period-input_width].cumsum(), False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_CUMSUM' + '_predictions',extract_result[train_period-input_width:val_period-input_width].cumsum(), False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_CUMSUM' + '_predictions', extract_result[val_period-input_width:test_period-input_width].cumsum(), False)\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_norm' + '_CUMSUM' + '_predictions', norm_result[0:train_period-input_width].cumsum(), False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_norm' + '_CUMSUM' + '_predictions',norm_result[train_period-input_width:val_period-input_width].cumsum(), False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_norm' + '_CUMSUM' + '_predictions', norm_result[val_period-input_width:test_period-input_width].cumsum(), False)\n",
    "                    colors.append('r')\n",
    "                if 'Water'.lower() in label.lower():\n",
    "                    phase = label.replace('_norm','')\n",
    "                    \n",
    "                    # CUMSUM of actual water phase production (independent of mode type)\n",
    "                    train_df_copy.insert(len(train_df_copy.keys()), phase + '_CUMSUM',train_df_copy[phase].cumsum(), False)\n",
    "                    val_df_copy.insert(len(val_df_copy.keys()), phase + '_CUMSUM',val_df_copy[phase].cumsum(), False)\n",
    "                    test_df_copy.insert(len(test_df_copy.keys()), phase + '_CUMSUM',test_df_copy[phase].cumsum(), False)\n",
    "                    \n",
    "                    if mode == 'many-to-one':\n",
    "                        norm_result = current_forecast[:,j]\n",
    "                        extract_result = current_forecast[:,j]*train_sigma[index][phase] + train_mu[index][phase]\n",
    "                        real_forecast.append(extract_result)\n",
    "                        # Extracting results for train, val, test periods to evaluate\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_predictions', extract_result[0:train_period-input_width], False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_predictions',extract_result[train_period-input_width:val_period-input_width], False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_predictions', extract_result[val_period-input_width:test_period-input_width], False)\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_norm' + '_predictions', norm_result[0:train_period-input_width], False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_norm' + '_predictions',norm_result[train_period-input_width:val_period-input_width], False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_norm' + '_predictions', norm_result[val_period-input_width:test_period-input_width], False)\n",
    "                        # Save CUMSUM (PREDICTIONS) for train, val, test periods to evaluate\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_CUMSUM' + '_predictions', extract_result[0:train_period-input_width].cumsum(), False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_CUMSUM' + '_predictions',extract_result[train_period-input_width:val_period-input_width].cumsum(), False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_CUMSUM' + '_predictions', extract_result[val_period-input_width:test_period-input_width].cumsum(), False)\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_norm' + '_CUMSUM' + '_predictions', norm_result[0:train_period-input_width].cumsum(), False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_norm' + '_CUMSUM' + '_predictions',norm_result[train_period-input_width:val_period-input_width].cumsum(), False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_norm' + '_CUMSUM' + '_predictions', norm_result[val_period-input_width:test_period-input_width].cumsum(), False)\n",
    "                    elif mode == 'many-to-many':\n",
    "                        if norm_method == 'z-score':\n",
    "                            norm_result = current_forecast[:,-1][:,j]\n",
    "                            extract_result = current_forecast[:,-1][:,j]*train_sigma[index][phase] + train_mu[index][phase]\n",
    "                            real_forecast.append(extract_result)\n",
    "                        elif norm_method == 'min-max':\n",
    "                            norm_result = current_forecast[:,-1][:,j]\n",
    "                            extract_result = current_forecast[:,-1][:,j]*(train_sigma[index][phase] - train_mu[index]) + train_mu[index]\n",
    "                            real_forecast.append(extract_result)\n",
    "                        # Extracting results for train, val, test periods to evaluate\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_predictions', extract_result[0:train_period-input_width], False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_predictions',extract_result[train_period-input_width:val_period-input_width], False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_predictions', extract_result[val_period-input_width:test_period-input_width], False)\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_norm' + '_predictions', norm_result[0:train_period-input_width], False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_norm' + '_predictions',norm_result[train_period-input_width:val_period-input_width], False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_norm' + '_predictions', norm_result[val_period-input_width:test_period-input_width], False)\n",
    "                        # Save CUMSUM (PREDICTIONS) for train, val, test periods to evaluate\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_CUMSUM' + '_predictions', extract_result[0:train_period-input_width].cumsum(), False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_CUMSUM' + '_predictions',extract_result[train_period-input_width:val_period-input_width].cumsum(), False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_CUMSUM' + '_predictions', extract_result[val_period-input_width:test_period-input_width].cumsum(), False)\n",
    "                        train_df_copy.insert(len(train_df_copy.keys()), phase + '_norm' + '_CUMSUM' + '_predictions', norm_result[0:train_period-input_width].cumsum(), False)\n",
    "                        val_df_copy.insert(len(val_df_copy.keys()), phase + '_norm' + '_CUMSUM' + '_predictions',norm_result[train_period-input_width:val_period-input_width].cumsum(), False)\n",
    "                        test_df_copy.insert(len(test_df_copy.keys()), phase + '_norm' + '_CUMSUM' + '_predictions', norm_result[val_period-input_width:test_period-input_width].cumsum(), False)\n",
    "                    colors.append('b')  \n",
    "            \n",
    "\n",
    "            # Append extracted results to evaluate\n",
    "            Forecast_Results_train.append(train_df_copy)\n",
    "            Forecast_Results_val.append(val_df_copy)\n",
    "            Forecast_Results_test.append(test_df_copy)\n",
    "            \n",
    "            # Concatonated to compute loss on combined train,val,test per single well\n",
    "            single_well_results_df = pd.concat([train_df_copy,val_df_copy,test_df_copy])\n",
    "            Forecast_Results.append(single_well_results_df)\n",
    "            \n",
    "            # Number of months in merged df (required for plots)\n",
    "            total_months = np.array(list(range(0,len(merged_df))))\n",
    "\n",
    "            # *** Plots and saves graph for predictions for phases in label_columns = ['oil','gas', water] ***\n",
    "            for i,label in enumerate(label_columns):\n",
    "                # Current phase to plot\n",
    "                phase = label.replace('_norm','')\n",
    "                \n",
    "                # Available loss types (except mape --> since y_label contains 0 values which causes division issues in this study)\n",
    "                loss_type = ['mse','rmse','mae','nmse','nrmse','nmape','nmae','wmape','mape'] \n",
    "                \n",
    "                # Train, Val, Test losses per well and per phase\n",
    "                train_loss = loss_compute(y_label = train_df_copy[phase + norm_values], y_pred = train_df_copy[phase + norm_values + '_predictions'], y_label_full = train_df_copy[phase + norm_values], loss_type = loss_type)\n",
    "                val_loss = loss_compute(y_label = val_df_copy[phase + norm_values], y_pred = val_df_copy[phase + norm_values + '_predictions'], y_label_full = val_df_copy[phase + norm_values], loss_type = loss_type)\n",
    "                test_loss = loss_compute(y_label = test_df_copy[phase + norm_values], y_pred = test_df_copy[phase + norm_values + '_predictions'], y_label_full = test_df_copy[phase + norm_values], loss_type = loss_type)\n",
    "                \n",
    "                # Calculate mse loss for the well (train,val,test combined)\n",
    "                single_well_loss = loss_compute(y_label = single_well_results_df[phase + norm_values], y_pred = single_well_results_df[phase + norm_values + '_predictions'], y_label_full = single_well_results_df[phase + norm_values], loss_type = loss_type)\n",
    "                \n",
    "                # Collect single well loss\n",
    "                API_DICT = collect_errors(API = current_well, losses = single_well_loss)\n",
    "                API_DICT.update({'phase': phase}) \n",
    "                Error_list.append(API_DICT)\n",
    "                \n",
    "                # Collect train,val,test losses\n",
    "                Train_API_DICT = collect_errors(API = current_well, losses = train_loss)\n",
    "                Train_API_DICT.update({'phase': phase}) \n",
    "                Train_Error_list.append(Train_API_DICT)\n",
    "                \n",
    "                Val_API_DICT = collect_errors(API = current_well, losses = val_loss)\n",
    "                Val_API_DICT.update({'phase': phase}) \n",
    "                Val_Error_list.append(Val_API_DICT)\n",
    "                \n",
    "                Test_API_DICT = collect_errors(API = current_well, losses = test_loss)\n",
    "                Test_API_DICT.update({'phase': phase}) \n",
    "                Test_Error_list.append(Test_API_DICT)\n",
    "                \n",
    "                # Figure setup\n",
    "                fig, ax1 = plt.subplots()\n",
    "\n",
    "                # Plot parameters\n",
    "                if split_loss == 'off':\n",
    "                    plt.title((phase + '\\n{}: (' + str(round(single_well_loss[plot_loss],3)) + ')').format(plot_loss.upper()), fontweight=\"bold\")\n",
    "                \n",
    "                if split_loss == 'on':                \n",
    "                # Possible plot title change\n",
    "                    plt.title((phase + '\\nTrain {}: (' + str(round(train_loss[plot_loss],3)) + ')' + '\\nVal {}: (' + str(round(val_loss[plot_loss],3)) + ')' + '\\nTest {}: (' + str(round(test_loss[plot_loss],3)) + ')').format(plot_loss.upper(),plot_loss.upper(),plot_loss.upper()), fontweight=\"bold\", fontsize = 15)\n",
    "\n",
    "                ax2 = ax1.twinx()\n",
    "                if logscale == 'on':\n",
    "                    ax1.set_yscale('log')\n",
    "                else:\n",
    "                    ax1.set_yscale('linear')\n",
    "\n",
    "                # Original Data\n",
    "                ax1.scatter(total_months,np.array(merged_df[phase]),label=phase, color = 'k')\n",
    "\n",
    "                # Predictions\n",
    "                ax1.plot(total_months[input_width:],real_forecast[i], color = colors[i], label = 'Predictions')\n",
    "\n",
    "                # Plot vertical lines for train,val, and test periods\n",
    "                ax1.axvline(x = train_period, color = 'orange', linestyle = '--', linewidth = 5, label = 'Train')\n",
    "                ax1.axvline(x = val_period, color = 'c', linestyle = '--', linewidth = 5, label = 'Validation')\n",
    "                ax1.axvline(x = test_period, color = 'm', linestyle = '--', linewidth = 5, label = 'Test')\n",
    "\n",
    "                # Plot control parameter (days on)\n",
    "                ax2.scatter(total_months[1:],merged_df['Days'][0:-1], s = 96, color = 'deeppink', marker = \"x\", label = 'Days_on')\n",
    "\n",
    "                # Label x and y axes\n",
    "                ax1.set_ylabel('Rate')\n",
    "                ax1.set_xlabel('Month')\n",
    "\n",
    "                ax2.set_ylabel('Days on (Control)')\n",
    "\n",
    "                fig.legend(loc=\"upper center\", fancybox=True, shadow=True, ncol=3)\n",
    "\n",
    "\n",
    "                # Save and close figures\n",
    "                plt.savefig(os.path.join(save_path,'API_' + str(current_well) + '_' + phase + '.png'))\n",
    "                plt.clf()\n",
    "                plt.close(fig=fig)\n",
    "    \n",
    "    # Concatonate all dataframes of all wells (extracted results to evaluate in postprocessing) - saved as an excel sheet\n",
    "    train_results = pd.concat(Forecast_Results_train)\n",
    "    val_results = pd.concat(Forecast_Results_val)\n",
    "    test_results = pd.concat(Forecast_Results_test)\n",
    "    per_well_results = pd.concat(Forecast_Results)\n",
    "    \n",
    "    # Create error dataframe per well (inclused train, val, test)\n",
    "    Error_data = pd.DataFrame(Error_list)\n",
    "    Train_Error_data = pd.DataFrame(Train_Error_list)\n",
    "    Val_Error_data = pd.DataFrame(Val_Error_list)\n",
    "    Test_Error_data = pd.DataFrame(Test_Error_list)\n",
    "    \n",
    "    saved_data = {'train_results':train_results, \n",
    "                  'val_results':val_results, \n",
    "                  'test_results':test_results,\n",
    "                  'single_well_results':per_well_results,\n",
    "                  'single_well_error': Error_data, \n",
    "                  'train_error':Train_Error_data, \n",
    "                  'val_error':Val_Error_data, \n",
    "                  'test_error':Test_Error_data} \n",
    "    \n",
    "    return saved_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "163c1033-e3a1-4141-9bda-6413c5b0fa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For easy histogram creation\n",
    "# Recommended (Example): dataframe = dataframe, column = ['Monthly Oil'], by = ['targetFormation','County'], bins = 'auto', alpha = 0.7, figsize = (15,15) \n",
    "def create_hist(dataframe = None, column = None, by = None, bins = None, alpha = None, figsize = None):\n",
    "    dataframe.hist(column = column, bins=bins, alpha=alpha, by=by, figsize = figsize, edgecolor='black',range=[df[column].min(), df[column].max()] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "214e2ba9-f3d6-4d04-be19-dce1349eaa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A convenient way to create models and change their hyperparameters (useful for OPTUNA studies)\n",
    "\n",
    "def create_model(final_layer_return_seq = True, seed = 0, initializer = 'orthogonal', loss='mse', metrics = ['mae','mape'], layers = None, dropout_final_layer = False, dropout_value = 0.2, recurrent_dropout = 'zeros', units = 30, model_type = None, Bi_directional = False, i = None, optimizer = None): #  c = None,\n",
    "    \n",
    "    # For globally reproducible random weights\n",
    "    tf.keras.utils.set_random_seed(seed=seed)\n",
    "    \n",
    "    # Initialize weights\n",
    "    if initializer == 'orthogonal':\n",
    "        initializer = 'orthogonal'\n",
    "    elif initializer == 'glorot_uniform':\n",
    "        initializer = 'glorot_uniform'\n",
    "    \n",
    "    # Control recurrent dropout (list of zeros by default)\n",
    "    if isinstance(recurrent_dropout, str): \n",
    "        if ((recurrent_dropout.lower() == 'zeros') or (recurrent_dropout.lower() == 'zero')) and Bi_directional == False:\n",
    "            recurrent_dropout = [0]*layers\n",
    "        elif ((recurrent_dropout.lower() == 'zeros') or (recurrent_dropout.lower() == 'zero')) and Bi_directional == True:\n",
    "            recurrent_dropout = [[0,0]]*layers\n",
    "        else:\n",
    "            assert (recurrent_dropout.lower() == 'zeros') or (recurrent_dropout.lower() == 'zero'), 'Provided string is not valid. Only valid option is \"zero\".'\n",
    "    elif not isinstance(recurrent_dropout, str):\n",
    "        recurrent_dropout = recurrent_dropout\n",
    "        \n",
    "    # Control units\n",
    "    assert isinstance(units,list) or isinstance(units, int), 'Only int and list of ints is valid for units variable.'\n",
    "    if isinstance(units, list):\n",
    "        assert all(isinstance(element, int) or isinstance(element, list) for element in units), 'Units list contains a string or float element. Units can only be integer elements or a list of integer elements [[int,int]] for Bi-directional RNN models'\n",
    "        units = units\n",
    "    elif isinstance(units, int) and Bi_directional == False:\n",
    "        units = [units]*layers\n",
    "    elif isinstance(units, int) and Bi_directional == True:\n",
    "        units = [[units,units]]*layers\n",
    "    \n",
    "    # Recalculate layers\n",
    "    layers = layers - 1\n",
    "    \n",
    "    # Main layer\n",
    "    if model_type == 'GRU':\n",
    "        main_layer = GRU\n",
    "    elif model_type == 'LSTM':\n",
    "        main_layer = LSTM\n",
    "    \n",
    "    # if model type is ANN\n",
    "    if model_type == 'Dense':\n",
    "        main_layer = Dense\n",
    "        if layers > 0:\n",
    "            # Initial Dense layer\n",
    "            x = main_layer(units = units[0], kernel_initializer=initializer, activation='sigmoid')(i)\n",
    "            units.pop(0) # remove first element after use\n",
    "\n",
    "            for num_layers in range(0,layers):\n",
    "                x = main_layer(units = units[num_layers], kernel_initializer=initializer, activation='sigmoid')(x)\n",
    "\n",
    "        elif layers == 0:\n",
    "            # Initial ANN layer\n",
    "            x = main_layer(units = units[0], kernel_initializer=initializer, activation='sigmoid')(i)\n",
    "            units.pop(0) # remove first element after use\n",
    "\n",
    "        # Optional dropout at the end\n",
    "        if dropout_final_layer == True:\n",
    "            x = Dropout(dropout_value)(x)\n",
    "\n",
    "        x = Dense(units=3, activation='sigmoid')(x)\n",
    "\n",
    "        # Create and compile the model\n",
    "        model = Model(inputs=i, outputs=x)\n",
    "        model.compile(optimizer=optimizer, loss = loss, metrics = metrics)\n",
    "    \n",
    "    elif model_type == 'LR': # A Linear Regression model using ANN (linear ANN)\n",
    "        main_layer = Dense\n",
    "        x = main_layer(units = 3, kernel_initializer=initializer, activation='linear')(i) # directly relate to outputs with no hidden layers when using LR\n",
    "        units.pop(0) # remove first element after use\n",
    "        \n",
    "        # Create and compile the model\n",
    "        model = Model(inputs=i, outputs=x)\n",
    "        model.compile(optimizer=optimizer, loss = loss, metrics = metrics)\n",
    "    \n",
    "    # if model is an RNN\n",
    "    elif model_type != 'Dense':\n",
    "\n",
    "        # Standard GRU or LSTM Option\n",
    "        if not Bi_directional:\n",
    "            if final_layer_return_seq == True:\n",
    "                # Initial RNN layer\n",
    "                x = main_layer(units = units[0], kernel_initializer=initializer, recurrent_dropout = recurrent_dropout[0], return_sequences=True)(i)\n",
    "                recurrent_dropout.pop(0) # remove first element after use\n",
    "                units.pop(0) # remove first element after use\n",
    "\n",
    "                for num_layers in range(0,layers):\n",
    "                    x = main_layer(units = units[num_layers], kernel_initializer=initializer, recurrent_dropout = recurrent_dropout[num_layers], return_sequences=True)(x)\n",
    "\n",
    "            elif final_layer_return_seq == False:\n",
    "                if layers > 0:\n",
    "                    # Initial RNN layer\n",
    "                    x = main_layer(units = units[0], kernel_initializer=initializer, recurrent_dropout = recurrent_dropout[0], return_sequences=True)(i)\n",
    "                    recurrent_dropout.pop(0) # remove first element after use\n",
    "                    units.pop(0) # remove first element after use\n",
    "\n",
    "                    for num_layers in range(0,layers):\n",
    "                        if (num_layers+1) < layers:\n",
    "                            x = main_layer(units = units[num_layers], kernel_initializer=initializer, recurrent_dropout = recurrent_dropout[num_layers], return_sequences=True)(x)\n",
    "                        elif (num_layers+1) == layers:\n",
    "                            x = main_layer(units = units[num_layers], kernel_initializer=initializer, recurrent_dropout = recurrent_dropout[num_layers], return_sequences=False)(x)\n",
    "\n",
    "                elif layers == 0:\n",
    "                    # Initial RNN layer\n",
    "                    x = main_layer(units = units[0], kernel_initializer=initializer, recurrent_dropout = recurrent_dropout[0], return_sequences=False)(i)\n",
    "                    recurrent_dropout.pop(0) # remove first element after use\n",
    "                    units.pop(0) # remove first element after use\n",
    "\n",
    "            # Optional dropout at the end\n",
    "            if dropout_final_layer == True:\n",
    "                x = Dropout(dropout_value)(x)\n",
    "\n",
    "            x = Dense(units=3, activation='sigmoid')(x)\n",
    "\n",
    "            # Create and compile the model\n",
    "            model = Model(inputs=i, outputs=x)\n",
    "            model.compile(optimizer=optimizer, loss = loss, metrics = metrics)\n",
    "\n",
    "        # Bi-directional Option (Bi-LSTM or Bi-GRU)\n",
    "        if Bi_directional:\n",
    "\n",
    "            # inputs\n",
    "            inputs = (Input(shape=(i.shape[1], i.shape[2])))\n",
    "\n",
    "            if final_layer_return_seq == True:\n",
    "                # forward and backward layers\n",
    "                forward_layer = main_layer(units = units[0][0], kernel_initializer=initializer, recurrent_dropout = recurrent_dropout[0][0], return_sequences=True)\n",
    "                backward_layer = main_layer(units = units[0][1], kernel_initializer=initializer, recurrent_dropout = recurrent_dropout[0][1], go_backwards=True, return_sequences=True)\n",
    "                # Initial RNN layer\n",
    "                outputs = Bidirectional(layer=forward_layer, backward_layer=backward_layer)(inputs = inputs)\n",
    "                recurrent_dropout.pop(0) # remove first element after use\n",
    "                units.pop(0) # remove first element after use\n",
    "\n",
    "                for num_layers in range(0,layers):\n",
    "                    outputs = (outputs)\n",
    "                    forward_layer = main_layer(units = units[num_layers][0], kernel_initializer=initializer, recurrent_dropout = recurrent_dropout[num_layers][0], return_sequences=True)\n",
    "                    backward_layer = main_layer(units = units[num_layers][1], kernel_initializer=initializer, recurrent_dropout = recurrent_dropout[num_layers][1], go_backwards=True, return_sequences=True)\n",
    "                    outputs = Bidirectional(layer=forward_layer, backward_layer=backward_layer)(outputs)\n",
    "            elif final_layer_return_seq == False:\n",
    "                if layers > 0:\n",
    "                    # forward and backward layers\n",
    "                    forward_layer = main_layer(units = units[0][0], kernel_initializer=initializer, recurrent_dropout = recurrent_dropout[0][0], return_sequences=True)\n",
    "                    backward_layer = main_layer(units = units[0][1], kernel_initializer=initializer, recurrent_dropout = recurrent_dropout[0][1], go_backwards=True, return_sequences=True)\n",
    "                    # Initial RNN layer\n",
    "                    outputs = Bidirectional(layer=forward_layer, backward_layer=backward_layer)(inputs = inputs)\n",
    "                    recurrent_dropout.pop(0) # remove first element after use\n",
    "                    units.pop(0) # remove first element after use\n",
    "\n",
    "                    for num_layers in range(0,layers):\n",
    "                        if (num_layers+1) < layers:\n",
    "                            outputs = (outputs)\n",
    "                            forward_layer = main_layer(units = units[num_layers][0], kernel_initializer=initializer, recurrent_dropout = recurrent_dropout[num_layers][0], return_sequences=True)\n",
    "                            backward_layer = main_layer(units = units[num_layers][1], kernel_initializer=initializer, recurrent_dropout = recurrent_dropout[num_layers][1], go_backwards=True, return_sequences=True)\n",
    "                            outputs = Bidirectional(layer=forward_layer, backward_layer=backward_layer)(outputs)\n",
    "                        elif (num_layers+1) == layers:\n",
    "                            outputs = (outputs)\n",
    "                            forward_layer = main_layer(units = units[num_layers][0], kernel_initializer=initializer, recurrent_dropout = recurrent_dropout[num_layers][0], return_sequences=False)\n",
    "                            backward_layer = main_layer(units = units[num_layers][1], kernel_initializer=initializer, recurrent_dropout = recurrent_dropout[num_layers][1], go_backwards=True, return_sequences=False)\n",
    "                            outputs = Bidirectional(layer=forward_layer, backward_layer=backward_layer)(outputs)\n",
    "\n",
    "                elif layers == 0:\n",
    "                    # forward and backward layers\n",
    "                    forward_layer = main_layer(units = units[0][0], kernel_initializer=initializer, recurrent_dropout = recurrent_dropout[0][0], return_sequences=False)\n",
    "                    backward_layer = main_layer(units = units[0][1], kernel_initializer=initializer, recurrent_dropout = recurrent_dropout[0][1], go_backwards=False, return_sequences=True)\n",
    "                    # Initial RNN layer\n",
    "                    outputs = Bidirectional(layer=forward_layer, backward_layer=backward_layer)(inputs = inputs)\n",
    "                    recurrent_dropout.pop(0) # remove first element after use\n",
    "                    units.pop(0) # remove first element after use\n",
    "\n",
    "\n",
    "            # Optional dropout at the end\n",
    "            if dropout_final_layer == True:\n",
    "                outputs = Dropout(dropout_value)(outputs)\n",
    "\n",
    "            \n",
    "            # Output layer\n",
    "            outputs = Dense(units=3, activation='sigmoid')(outputs)\n",
    "                \n",
    "\n",
    "\n",
    "            # Create and compile the model\n",
    "            model = Model(inputs=inputs, outputs=outputs)\n",
    "            model.compile(optimizer=optimizer, loss = loss, metrics = metrics)\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
